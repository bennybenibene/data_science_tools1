{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7765c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a74399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Dataframe from draws from random normal dist, specifying index and cols\n",
    "df = DataFrame(np.random.randn(10, 5),\n",
    "               index=[i for i in 'IN OH MI KY IL CO WY NM CA OR'.split(' ')],\n",
    "               columns=['feat_' + str(i) for i in range(1, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae38c76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_data = {'state': [i for i in 'IN OH MI KY IL CO WY NM CA OR'.split(' ')],\n",
    "             'is_big': [False, True, True, False, np.nan, True, False, True, True, True],\n",
    "             'nickname': ['Hoosier', 'Buckeye', 'Wolverine', 'Bluegrass', 'Prairie',\n",
    "                          'Centennial', 'Equality', 'Enchantment', 'Golden', 'Beaver']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0a0a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice we're not setting an index value here.\n",
    "df2 = DataFrame(more_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5a321",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"as an aside, note that we can use the from_dict() method here as well.\n",
    "this method takes an optional argument, orient'.  If keys should be columns\n",
    "pass 'columns' to orient (default behavior).  If keys should be row indices \n",
    "pass 'index' to this parameter.  See below...\n",
    "\"\"\"\n",
    "\n",
    "df2 = DataFrame.from_dict(more_data, orient='index')\n",
    "print(df2)\n",
    "\n",
    "df2 = DataFrame.from_dict(more_data, orient='columns')\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00195b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's join these DataFrames\n",
    "data = df.merge(df2, how='left', left_index=True, right_on='state')\n",
    "\n",
    "# Notice the resulting indices\n",
    "print(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112b6e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's merge them again, this time we'll specify indices for df2.\n",
    "# This will introduce some redundancy into our DataFrame\n",
    "# df2.set_index('state', inplace=True) is a better alternative.\n",
    "df2 = DataFrame(more_data, index=more_data['state'])\n",
    "df2.drop(columns='state', inplace=True)\n",
    "\n",
    "# Executing the merge again.\n",
    "data = df.merge(df2, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea5d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now notice the resulting indices.\n",
    "print(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ed7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perhaps we want the state code as a feature and not the index\n",
    "data.reset_index(drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10abba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the name of the state code feature\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855b8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c28eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming cols inplace\n",
    "data.rename(columns={'index': 'state'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc9f8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting index back to state code\n",
    "data.set_index('state', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49823df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that the index now has a name attribute\n",
    "print(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08094dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add another feature.\n",
    "data['year'] = '1997'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32327031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make another df to concat our first dataframe\n",
    "another_df = DataFrame(np.random.randn(10, 5),\n",
    "                       index=[i for i in 'IN OH MI KY IL CO WY NM CA OR'.split(' ')],\n",
    "                       columns=['feat_' + str(i) for i in range(1, 6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ed728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "another_df = another_df.merge(df2, how='left', left_index=True, right_index=True)\n",
    "\n",
    "another_df['year'] = '2001'\n",
    "\n",
    "# and we'll add another column to this df.\n",
    "another_df['quarter'] = 'q1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003a2b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the two dataframes\n",
    "# axis arg is 0 by default\n",
    "final_df = pd.concat([data, another_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2fc5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e829668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna for quarter...this wasn't a feature in big_df.\n",
    "final_df['quarter'].fillna(value='q1', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd13e0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we're going to double the length of our primary DataFrame.\n",
    "# Be cautious with aliasing...\n",
    "copied = final_df\n",
    "\n",
    "copied['quarter'] = 'q2' # chaning the quarter column value\n",
    "\n",
    "print(final_df['quarter']) # insepect the original column of the df from which we copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca73334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That wasn't good.  Let's change the 'quarter' vale of final_df back to 'q1'\n",
    "final_df['quarter'] = 'q1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .copy() instead. Copy takes one optional argument, deep.\n",
    "# The param is True by default. deep=False returns a shallow copy.\n",
    "copied = final_df.copy()\n",
    "copied['quarter'] = 'q2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54286a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirming that the change to copied\n",
    "# had no impact on final_df\n",
    "print(final_df['quarter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f560d339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating over the feature columns and modifying them randomly\n",
    "for i in ['feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5']:\n",
    "    copied[i] = np.vectorize(lambda arg: np.random.randn())(copied[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be7d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([final_df, copied], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc34dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pivot this and join an aggregate field to our final_df.\n",
    "# Say, for some reason, we need the max across annual quarters of the feature values by year\n",
    "pivoted = final_df.pivot_table(index=final_df.index,\n",
    "                               columns='year',\n",
    "                               values=['feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5'],\n",
    "                               aggfunc=np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48905a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now have MultiIndexed columns where column indices are represented as tuples\n",
    "print(pivoted.columns)\n",
    "print(pivoted.index)\n",
    "print(pivoted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aaab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiIndexed rows and columns are totally fine, but what if we want to flatten them out...\n",
    "pivoted.columns = ['_'.join(col) for col in pivoted.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd405535",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pivoted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bcc5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe it makes more sense to pivot this way, if we want to join these data.\n",
    "pivoted = final_df.pivot_table(index=[final_df.index, 'year'],\n",
    "                               values=['feat_1', 'feat_2', 'feat_3', 'feat_4', 'feat_5'],\n",
    "                               aggfunc=np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b991c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have MultiIndexed rows.\n",
    "print(pivoted.columns)\n",
    "print(pivoted.index)\n",
    "print(pivoted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c18a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick aside:\n",
    "# we could aggregate over components of the multi-index.\n",
    "# note that the level keyword in Series and DataFrame aggregationis deprecated. Use groupby instead.\n",
    "# pivoted.sum(level='year')\n",
    "\n",
    "pivoted.groupby(level=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf3ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that our pivoted data contains the same feature names as were in our primary data\n",
    "# If we merge our pivoted data to our original dataset Pandas will handle this by\n",
    "# appending an _x and _y to these feature names. We'll change these names first.\n",
    "pivoted.columns = [i + '_ann_max' for i in pivoted.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1645aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging pivoted data to our final_df\n",
    "final_df = final_df.merge(pivoted,\n",
    "                          how='left',\n",
    "                          left_on=[final_df.index, 'year'],\n",
    "                          right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9d2f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting index\n",
    "final_df.sort_index(ascending=True,\n",
    "                    inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb811dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5370aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting on additional cols\n",
    "final_df.sort_values(['year', 'quarter'],\n",
    "                     ascending=[False, True],\n",
    "                     inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0691de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df[['year', 'quarter']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e894040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe we want to drop records with a certain nickname\n",
    "# we can send the indices of the data with certain properties to a list\n",
    "location = final_df.index[(final_df['nickname'].isin(['Buckeye', 'Bluegrass']))].tolist()\n",
    "\n",
    "#final_df.drop(index=location, inplace=True)\n",
    "# we'll hold off on dropping them for now though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb2182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with missing values\n",
    "\n",
    "#final_df.dropna(axis=0, inplace=True)\n",
    "\n",
    "# let's hold off on this as well.  Instead we'll impute.\n",
    "\n",
    "# first let's inspect which columns have missing data\n",
    "final_df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e0b9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use this bool array to subset our actual column names.\n",
    "# this is much easier to see columns with missing data.\n",
    "missing = final_df.columns[final_df.isnull().any()].tolist()\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ec7228",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in  missing:\n",
    "    final_df[col].fillna(value='maybe', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37457400",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in final_df['is_big'].unique():\n",
    "    final_df[i] = np.vectorize(lambda x: 1 if x == True else 0)(final_df['is_big'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c431c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at this DataFrame\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f70eae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do some groupby operations.\n",
    "# This just establishes a groupby object\n",
    "gb1 = final_df.groupby(['is_big'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea52fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can call aggregation functions on this object\n",
    "gb1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d26c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb1[['feat_1', 'feat_2']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4b0c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb1[['feat_1', 'feat_2']].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63255fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirming the feat_1 min for is_big = False\n",
    "final_df.loc[final_df['is_big'] == False, 'feat_1'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2432d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the add index parameter allows you to store the index arrays as columns\n",
    "gb2 = final_df.groupby(['quarter', 'nickname'], as_index=False).sum()\n",
    "print(gb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ae5564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the same as this operation, essentially\n",
    "gb2 = final_df.groupby(['quarter', 'nickname']).sum().reset_index()\n",
    "print(gb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c5d1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = final_df[['nickname', 'quarter', 'is_big', 'feat_1', 'feat_2']].copy()\n",
    "sub_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa970c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c7e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb3 = sub_df.groupby(['nickname'])\n",
    "for grouped in gb3:\n",
    "    print(grouped)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff4ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the groups from the groupby object\n",
    "gb3.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0737a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_group method to return the DataFrame for a group by name.\n",
    "gb3.get_group('Beaver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d0b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a trivial function to apply to our groupby object\n",
    "def do_something(df):\n",
    "    \n",
    "    if any(df['is_big'] == True):\n",
    "        new_value = df['feat_1'].max() + df['feat_2'].max()\n",
    "    else:\n",
    "        new_value = 0\n",
    "    return new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0e82a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb3.apply(do_something)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7b3d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's modify this to use the group and associated df within the function\n",
    "def do_something_new(grouped):\n",
    "    \n",
    "    group, df = grouped\n",
    "    \n",
    "    if any(df['is_big'] == True):\n",
    "        new_value = df['feat_1'].max() + df['feat_2'].max()\n",
    "    else:\n",
    "        new_value = 0\n",
    "    return {group: new_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be30ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing results in a list\n",
    "new_scores = [do_something_new(i) for i in gb3]\n",
    "for i in new_scores:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0c3fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "new_dict = defaultdict(str)\n",
    "for i in gb3:\n",
    "    new_dict.update(do_something_new(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3255afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame.from_dict(new_dict, orient='index', columns=['new_feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927c4305",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = pd.read_csv('https://nces.ed.gov/ipeds/datacenter/data/HD2021.zip', \n",
    "                    compression='zip',\n",
    "                    encoding=\"ISO-8859-1\")\n",
    "enr = pd.read_csv('https://nces.ed.gov/ipeds/datacenter/data/EFFY2021.zip',\n",
    "                  compression='zip',\n",
    "                  encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a24219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "enr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9da589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tools_venv",
   "language": "python",
   "name": "tools_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
