{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "716a593a",
   "metadata": {},
   "source": [
    "## COMP 4447: Week 8 Live Session\n",
    "## _Feature Engineering_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9155fe6c",
   "metadata": {},
   "source": [
    "### We've all heard this before, but accessing, cleaning and preparing the data for analysis is the most time-consuming part of a data science role.\n",
    "\n",
    "### However, being thorough and creative during these aspects of the process will have a huge impact on outcomes. Good features can tap into useful information and set your model apart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915b7385",
   "metadata": {},
   "source": [
    "<img src=\"https://pbs.twimg.com/media/EgN0LhoVoAAetP2.png\" width=\"500\" height=\"500\"/>\n",
    "<img src=\"https://rapidminer.com/wp-content/uploads/2022/04/data-prep-approach-time.png\" width=\"500\" height=\"500\"/>\n",
    "<img src=\"https://pbs.twimg.com/media/CwVwtGzUUAAM5x0.jpg\" width=\"500\" height=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a075f644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values....most ML and statistical models don't handle these well.\n",
    "# Assessing Missingness (MAR, MCAR, MNAR)\n",
    "# Imputation: central tendency and estimation approaches\n",
    "# Listwise deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba7be9d",
   "metadata": {},
   "source": [
    "#### There are different types of missingness and your approach to handling them can be informed by the type of missingness\n",
    "\n",
    "#### In general, we have the following options for handling missing data.\n",
    "\n",
    "- Listwise deletion\n",
    "\n",
    "- Imputation (for continuous and categorical data)\n",
    "\n",
    "- Using algorithms that are robust to missingness\n",
    "\n",
    "- Imputing predicted missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efa921b",
   "metadata": {},
   "source": [
    "## There are different varieties of missingness, referred to as missingness mechanisms...the names are a bit misleading.\n",
    "\n",
    "- __Missing Completely at Random (MCAR):__\n",
    "    \n",
    "    This is the best case scenario...and also the easiest to understand. Missingness is due to a random process and doesn't depend on any of the other observed variables. There is no evidence that there is a systematic explanation for the missing data.  Missing data casess are essentially a random sample of our dataset.  The only consequence is that we have fewer data to model.  \n",
    "\n",
    "    The misssingness doesn't introduce bias into our model.\n",
    "\n",
    "    Can be tested.\n",
    "    \n",
    "\n",
    "- __Missing at Random (MAR):__\n",
    "    \n",
    "    Here the missingness is conditioned upon another variable, but does not depend on the missing values.  For example, if you're utilizing survey data and observe that the missingness on a certain item (say job compensation) is associated with age, then the missingness for that item is conditioned upon age.  In this case compensation itself isn't impacting the missingness of compensation information, but perhaps older people are less likely to still be working, and we can expalin the missingness of compensation information based on their age.  \n",
    "    \n",
    "    Introduces bias, but we can control for it.\n",
    "    \n",
    "    Can be tested.\n",
    "\n",
    "\n",
    "- __Missing Not at Random (MNAR)__\n",
    "\n",
    "    This is the worst case. The missingness depends on the actual missing value. Let's use the compensation example again. Say we observe missing data for a measure of job compensation but find no association between missingness and age or any other features in our dataset.  However, maybe people who are paid less are less likely to offer up their compensation information.  In this case the missingness of compensation depends on the missing values themselves, so the data in this case would be considered MNAR.  The tricky part is, if we don't know the missing values, then we can't directly evaluate whether or not those values are associated with missingness.\n",
    "    \n",
    "    Introduces bias.  Harder to control for.\n",
    "    \n",
    "    Can't be directly tested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e26e260",
   "metadata": {},
   "source": [
    "## Testing for MCAR vs MAR\n",
    "\n",
    "A common approach here is to dummy code based on missingness and then perform t-tests or chi-square tests against our other features.  If we don't observe differential outcomes based on our missingness indicator, then we can assume that the data are missing completely at random.  If we observe differential outcomes, then we know that the data are missing at random or missing not at random. This could involve many comparisons depending on the amount of missing data you have, so it can be a good idea to use an adjustment for multiple comparisons such as Bonferroni.  Little's MCAR test computes a single test statistic by doing all of these operations and compares MAR vs MCAR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b10d52f",
   "metadata": {},
   "source": [
    "## How does the missingness mechanism impact handling of missing data?\n",
    "\n",
    "If your data are MCAR then you can comfortably listwise delete records with missing data.  Encoding missing values with an indicator, such as -1, can be helpful if you're using a tree based approach, as these values can be handled by the algorithm.  Simple single imputation techniques such as mean or median imputation can be used for MAR or MNAR.  Using regression (also single imputation) to estimate the values of the missing data using the other features in your dataset is a more sophisticated approach. Multiple imputation is probably the most sophisticated approach and involves making multiple imputed datasets using the distribution of the predicted value.  Model coefficients from multiple imputation can then be pooled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86421195",
   "metadata": {},
   "source": [
    "##  Caveats about missingness\n",
    "\n",
    "Assessment of missingness, especially at the level of MAR and MNAR requires domain knowledge and contextural information.  Rigorously dealing with missingness mechanisms is more important in clinical situtations.  Oftentimes we're primarily concerned with making accurate classifications or estimates, so if our models have good out-of-sample performance characteristics, then we can usually handle missingness as best we can and just except not fully understanding or controlling for the mechanisms. \n",
    "\n",
    "In many contexts, missing data just isn't that common or if it is, we know it's due to process or instrumentation. Say we're looking at transactional data collected from an internal system.  This is common and it's a much easier to diagnose missingness within this context.  Any time we're using data in which a subject, respondent or individual has to be engaged in the act of offering or reporting data we're much more likely to encounter missing data that has a more nebulous cause."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3a2e4f",
   "metadata": {},
   "source": [
    "#### A note about deletion\n",
    "\n",
    "We can use columnwise or listwise deletion. As a rule of thumb, we wouldn't want to use features that are missing more than 40% of their data.  Removal of rows is more common. Sometimes we set a threshold for record retention based on the number of missing values across all features.  Other times we consider record retention based on missingness for a single feature. \n",
    "\n",
    "Typically we want to avoid this strategy as it results in the loss of useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b6a55a",
   "metadata": {},
   "source": [
    "### One Hot Encoding: Reserved for strictly categorical data.\n",
    "\n",
    "Although you can do many of these tranformations in pandas, we'll focus on transformations using sklearn, as it's likely the tool you'll initially use for model-building, and when doing cross-validation it's easier to build a pipeline to apply your\n",
    "tranformation steps using built-in sklearn utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "556161a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Missouri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coloroado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coloroado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Coloroado</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        state\n",
       "0    Missouri\n",
       "1   Coloroado\n",
       "2   Coloroado\n",
       "3  California\n",
       "4   Coloroado"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we'll create a df with a single categorical feature just for example purposes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "np.random.seed(51)\n",
    "\n",
    "st = ['California', 'Coloroado', 'Missouri', 'Indiana']\n",
    "\n",
    "df = DataFrame({'state': [st[i] for i in np.random.randint(low=0, high=4, size=50)]})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60bd8373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oh = OneHotEncoder(sparse=True)\n",
    "\n",
    "# as you saw in the asynch there is a fit_tranform() method, but using the fit()\n",
    "# and transform() methods on their own is helpful when working in pipelines.\n",
    "# Here is an example of using those methods.\n",
    "\n",
    "oh.fit(df[['state']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d70613df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['state_California' 'state_Coloroado' 'state_Indiana' 'state_Missouri']\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 3)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 1)\t1.0\n",
      "  (3, 0)\t1.0\n",
      "  (4, 1)\t1.0\n",
      "  (5, 1)\t1.0\n",
      "  (6, 0)\t1.0\n",
      "  (7, 1)\t1.0\n",
      "  (8, 3)\t1.0\n",
      "  (9, 0)\t1.0\n",
      "  (10, 1)\t1.0\n",
      "  (11, 1)\t1.0\n",
      "  (12, 3)\t1.0\n",
      "  (13, 3)\t1.0\n",
      "  (14, 1)\t1.0\n",
      "  (15, 0)\t1.0\n",
      "  (16, 3)\t1.0\n",
      "  (17, 0)\t1.0\n",
      "  (18, 0)\t1.0\n",
      "  (19, 0)\t1.0\n",
      "  (20, 2)\t1.0\n",
      "  (21, 2)\t1.0\n",
      "  (22, 3)\t1.0\n",
      "  (23, 0)\t1.0\n",
      "  (24, 3)\t1.0\n",
      "  (25, 0)\t1.0\n",
      "  (26, 3)\t1.0\n",
      "  (27, 3)\t1.0\n",
      "  (28, 1)\t1.0\n",
      "  (29, 1)\t1.0\n",
      "  (30, 1)\t1.0\n",
      "  (31, 0)\t1.0\n",
      "  (32, 2)\t1.0\n",
      "  (33, 0)\t1.0\n",
      "  (34, 0)\t1.0\n",
      "  (35, 0)\t1.0\n",
      "  (36, 3)\t1.0\n",
      "  (37, 1)\t1.0\n",
      "  (38, 0)\t1.0\n",
      "  (39, 0)\t1.0\n",
      "  (40, 1)\t1.0\n",
      "  (41, 1)\t1.0\n",
      "  (42, 3)\t1.0\n",
      "  (43, 2)\t1.0\n",
      "  (44, 2)\t1.0\n",
      "  (45, 2)\t1.0\n",
      "  (46, 0)\t1.0\n",
      "  (47, 3)\t1.0\n",
      "  (48, 2)\t1.0\n",
      "  (49, 2)\t1.0\n"
     ]
    }
   ],
   "source": [
    "# Now we'll call the transform method from the fitted encoding.\n",
    "\n",
    "# Just so we understand the sparse param, let's look at a sparse scipy matrix.\n",
    "\n",
    "sparse_matrix = oh.transform(df[['state']])\n",
    "\n",
    "print(oh.get_feature_names_out()) # feature names\n",
    "\n",
    "print(type(sparse_matrix)) # it's a scipy object\n",
    "print(sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8463f47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "dense_matrix = csr_matrix.todense(oh.transform(df[['state']]))\n",
    "print(dense_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f54b10f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>state_California</th>\n",
       "      <th>state_Coloroado</th>\n",
       "      <th>state_Indiana</th>\n",
       "      <th>state_Missouri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Missouri</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coloroado</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coloroado</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>California</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Coloroado</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        state  state_California  state_Coloroado  state_Indiana  \\\n",
       "0    Missouri               0.0              0.0            0.0   \n",
       "1   Coloroado               0.0              1.0            0.0   \n",
       "2   Coloroado               0.0              1.0            0.0   \n",
       "3  California               1.0              0.0            0.0   \n",
       "4   Coloroado               0.0              1.0            0.0   \n",
       "\n",
       "   state_Missouri  \n",
       "0             1.0  \n",
       "1             0.0  \n",
       "2             0.0  \n",
       "3             0.0  \n",
       "4             0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh = OneHotEncoder(sparse=False)\n",
    "df[oh.get_feature_names_out()] = oh.fit_transform(df[['state']])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "73f0ca0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>state_California</th>\n",
       "      <th>state_Coloroado</th>\n",
       "      <th>state_Indiana</th>\n",
       "      <th>state_Missouri</th>\n",
       "      <th>state_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coloroado</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>California</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Coloroado</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        state  state_California  state_Coloroado  state_Indiana  \\\n",
       "0         NaN                 0                0              0   \n",
       "1         NaN                 0                0              0   \n",
       "2   Coloroado                 0                1              0   \n",
       "3  California                 1                0              0   \n",
       "4   Coloroado                 0                1              0   \n",
       "\n",
       "   state_Missouri  state_nan  \n",
       "0               0          1  \n",
       "1               0          1  \n",
       "2               0          0  \n",
       "3               0          0  \n",
       "4               0          0  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's introduce a misssing value in our state column and run this again.\n",
    "\n",
    "df.iloc[0, 0] = np.nan\n",
    "\n",
    "oh = OneHotEncoder(sparse=False, dtype=int)\n",
    "df[oh.get_feature_names_out()] = oh.fit_transform(df[['state']])\n",
    "df.head()\n",
    "\n",
    "# notice we get an encoding for nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "46f1f3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coloroado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Coloroado</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        state\n",
       "0         NaN\n",
       "1         NaN\n",
       "2   Coloroado\n",
       "3  California\n",
       "4   Coloroado"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can avoid this by passing in a list of valid categories when initializing our encoding object.\n",
    "# and setting the handle_unknown param to 'ignore'\n",
    "\n",
    "# first we'll get rid of the dummy vars we've already created.\n",
    "df = df[['state']].copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "91322acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>state_California</th>\n",
       "      <th>state_Coloroado</th>\n",
       "      <th>state_Missouri</th>\n",
       "      <th>state_Indiana</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coloroado</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>California</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Coloroado</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        state  state_California  state_Coloroado  state_Missouri  \\\n",
       "0         NaN                 0                0               0   \n",
       "1         NaN                 0                0               0   \n",
       "2   Coloroado                 0                1               0   \n",
       "3  California                 1                0               0   \n",
       "4   Coloroado                 0                1               0   \n",
       "\n",
       "   state_Indiana  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we ignore any values not explicitly specified when instantiating our OneHotEncoder.\n",
    "\n",
    "oh = OneHotEncoder(categories=[st], sparse=False, dtype=int, handle_unknown='ignore')\n",
    "df[oh.get_feature_names_out()] = oh.fit_transform(df[['state']])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca46b0ec",
   "metadata": {},
   "source": [
    "#### Note that with something like state, if we had full representation of all states we'd blow up our dimensionality very quickly by one-hot encoding everything.  Perhaps we're only interested in certain states, or maybe there's an attribute associated with the state that we could instead model...for example population, costal designation, gdp, distance from some point of concern. Perhaps state isn't important at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5fd58b",
   "metadata": {},
   "source": [
    "### Ordinal Data\n",
    "\n",
    "Note that when transforming exogenous features (predictors), it is recommended to use OrdinalEncoder instead of LabelEncoder, as the latter is intended for endogenous (target) variables.  OrdinalEncoder fits data with the shape (n_samples, n_features) while LabelEncoder only fits data with the shape (n_samples,).  You could use LabelEncoder in a loop, but OrdinalEncoder is preferred. \n",
    "\n",
    "When working in sklearn you'll notice that the features are usually considered as 2D arrays (n_sample, n_features) while the target is usually considered as a 1D array (n_samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "acfd5e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "# (4,) 1D array\n",
    "print(np.array([1, 2, 3, 4]).shape)\n",
    "\n",
    "\n",
    "print(np.array([[1],\n",
    "         [2],\n",
    "         [3],\n",
    "         [4]]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84fa0788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating1</th>\n",
       "      <th>rating2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Low</td>\n",
       "      <td>Very Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>High</td>\n",
       "      <td>Very Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>High</td>\n",
       "      <td>Very High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very High</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>High</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rating1    rating2\n",
       "0        Low   Very Low\n",
       "1       High   Very Low\n",
       "2       High  Very High\n",
       "3  Very High        Low\n",
       "4       High       High"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we'll create a df with a single categorical feature just for example purposes\n",
    "np.random.seed(51)\n",
    "\n",
    "rt = ['Very High', 'High', 'Low', 'Very Low']\n",
    "rt2 = ['Very High', 'High', 'Low', 'Very Low', 'ugh']\n",
    "\n",
    "df = DataFrame({'rating1': [rt[i] for i in np.random.randint(low=0, high=4, size=50)],\n",
    "               'rating2': [rt2[i] for i in np.random.randint(low=0, high=5, size=50)]})\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a54116a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n",
      "(50, 1)\n"
     ]
    }
   ],
   "source": [
    "# in terms of passsing the correct shapes, notice the following behavior.\n",
    "print(df['rating1'].shape)\n",
    "print(df[['rating1']].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4644c437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      rating1    rating2  rating_1_tr  rating_2_tr\n",
      "0         Low   Very Low          2.0          3.0\n",
      "1        High   Very Low          1.0          3.0\n",
      "2        High  Very High          1.0          0.0\n",
      "3   Very High        Low          0.0          2.0\n",
      "4        High       High          1.0          1.0\n",
      "5        High   Very Low          1.0          3.0\n",
      "6   Very High  Very High          0.0          0.0\n",
      "7        High       High          1.0          1.0\n",
      "8         Low       High          2.0          1.0\n",
      "9   Very High        ugh          0.0          NaN\n",
      "10       High       High          1.0          1.0\n",
      "11       High       High          1.0          1.0\n",
      "12        Low   Very Low          2.0          3.0\n",
      "13        Low   Very Low          2.0          3.0\n",
      "14       High        Low          1.0          2.0\n",
      "15  Very High   Very Low          0.0          3.0\n",
      "16        Low        ugh          2.0          NaN\n",
      "17  Very High       High          0.0          1.0\n",
      "18  Very High       High          0.0          1.0\n",
      "19  Very High  Very High          0.0          0.0\n",
      "20   Very Low   Very Low          3.0          3.0\n",
      "21   Very Low   Very Low          3.0          3.0\n",
      "22        Low   Very Low          2.0          3.0\n",
      "23  Very High        ugh          0.0          NaN\n",
      "24        Low        ugh          2.0          NaN\n",
      "25  Very High        Low          0.0          2.0\n",
      "26        Low        ugh          2.0          NaN\n",
      "27        Low  Very High          2.0          0.0\n",
      "28       High  Very High          1.0          0.0\n",
      "29       High   Very Low          1.0          3.0\n",
      "30       High       High          1.0          1.0\n",
      "31  Very High        ugh          0.0          NaN\n",
      "32   Very Low        ugh          3.0          NaN\n",
      "33  Very High        Low          0.0          2.0\n",
      "34  Very High       High          0.0          1.0\n",
      "35  Very High        ugh          0.0          NaN\n",
      "36        Low        ugh          2.0          NaN\n",
      "37       High  Very High          1.0          0.0\n",
      "38  Very High        ugh          0.0          NaN\n",
      "39  Very High  Very High          0.0          0.0\n",
      "40       High        ugh          1.0          NaN\n",
      "41       High        ugh          1.0          NaN\n",
      "42        Low       High          2.0          1.0\n",
      "43   Very Low        Low          3.0          2.0\n",
      "44   Very Low  Very High          3.0          0.0\n",
      "45   Very Low        ugh          3.0          NaN\n",
      "46  Very High        Low          0.0          2.0\n",
      "47        Low        ugh          2.0          NaN\n",
      "48   Very Low        ugh          3.0          NaN\n",
      "49   Very Low        Low          3.0          2.0\n"
     ]
    }
   ],
   "source": [
    "# Ordinal Data\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# using the default categories='auto' will pickup unknown or unwanted categories.\n",
    "oe = OrdinalEncoder(categories=[rt, rt], \n",
    "                    dtype=float,\n",
    "                    handle_unknown='use_encoded_value',\n",
    "                    unknown_value=np.nan) \n",
    "\n",
    "# note that there is newer param called encoded_missing_value. \n",
    "# You can set this to np.nan, but the dtype must be float if we use np.nan here.\n",
    "\n",
    "# this should probably work if you're using a recent sklearn install\n",
    "# but get_feature_names_out() hasn't been implemented on all tranformers yet.\n",
    "\n",
    "#df[oe.get_feature_names_out()] = oe.fit_transform(df[['rating1', 'rating2']])\n",
    "\n",
    "df[['rating_1_tr', 'rating_2_tr']] = oe.fit_transform(df[['rating1', 'rating2']])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf3bcde",
   "metadata": {},
   "source": [
    "### Scaling: Standardization vs Normalization\n",
    "\n",
    "We'll cover this in the asynch next week, but it's useful to talk about before then.  Many algorithms will require scaled data. This is particularly imoprtant in regression and distance based approaches such as K Nearest Neighbors, as raw variables on a different scale can exert an undue impact on our model.  \n",
    "\n",
    "Standardization and normalization are both types of scaling.  Standardization, or z scoring, is the act of subtracting the feature mean from each observation and dividing by the standard deviation. It results in a distribution with a mean of 0 and standard deviation of 1.  Standardization is unbounded and is best used with Gaussian data.  Standardization is more robust to outliers than min-max scaling.\n",
    "\n",
    "Normalization is typically used to desribe min-max scaling where we transform observations by subtracting the feature minimum and divding by the feature range.  Normalization scales the featurs values between 0 and 1 (bounded).  It's best used when your algorithm doesn't have any assumptions about the distribution of the input features.\n",
    "\n",
    "Unnecessary for tree based models, but still potentially a good idea\n",
    "\n",
    "Generally it's not necessary to scale your target, but it may be useful in some situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3f3c42c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8.659739592076221e-17\n",
      "1.005037815259212\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = DataFrame({'feat': np.random.rand(100)*100})\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "df['feat_z'] = ss.fit_transform(df[['feat']])\n",
    "\n",
    "print(df['feat_z'].mean())\n",
    "print(df['feat_z'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b56440f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mm = MinMaxScaler()\n",
    "\n",
    "df['feat_norm'] = mm.fit_transform(df[['feat']])\n",
    "\n",
    "print(df['feat_norm'].max())\n",
    "print(df['feat_norm'].min())\n",
    "\n",
    "# Note that sklearn Normalizer() operates row-wise and scales samples to unit norm.  We're concerned with normalizing columns\n",
    "# at the moment and should use MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ae8749",
   "metadata": {},
   "source": [
    "### Creative Feature Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2118b5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a relatively simple dataset.  Take a few minutes to explore and transform the data appropriately.\n",
    "#  Plot  the distribution of the raw wage column.  Apply a log tranformation on this column and plot it again.\n",
    "\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/selva86/datasets/master/Wage.csv\")\n",
    "\n",
    "# It would be unusual to have this feature created already.  We'll drop it and recreate later.\n",
    "data.drop('logwage', axis=1, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tools_venv",
   "language": "python",
   "name": "tools_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
